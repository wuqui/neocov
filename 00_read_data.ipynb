{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp read_data\n",
    "# all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENTS_DIR = '../data/comments/by_date/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_comments_paths_year(COMMENTS_DIR, YEAR):\n",
    "    comments_dir_path = Path(COMMENTS_DIR)\n",
    "    comments_paths = list(comments_dir_path.glob(f'{YEAR}*.csv'))\n",
    "    return comments_paths    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('../data/comments/by_date/2019-05-07_21:11:36___2019-05-07_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-08-07_21:12:15___2019-08-07_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-07-14_21:06:51___2019-07-14_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-07-01_21:59:59___2019-07-01_21:19:55.csv'),\n",
       " Path('../data/comments/by_date/2019-05-14_21:15:37___2019-05-14_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-06-07_21:17:11___2019-06-07_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-02-01_22:59:59___2019-02-01_22:02:38.csv'),\n",
       " Path('../data/comments/by_date/2019-02-07_22:06:26___2019-02-07_22:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-06-01_21:59:59___2019-06-01_21:09:37.csv'),\n",
       " Path('../data/comments/by_date/2019-11-07_22:24:23___2019-11-07_22:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-03-14_22:06:05___2019-03-14_22:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-10-07_21:17:33___2019-10-07_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-02-14_22:04:57___2019-02-14_22:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-12-07_22:29:19___2019-12-07_22:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-08-01_21:59:59___2019-08-01_21:13:12.csv'),\n",
       " Path('../data/comments/by_date/2019-03-01_22:59:59___2019-03-01_22:04:58.csv'),\n",
       " Path('../data/comments/by_date/2019-04-19_21:03:20___2019-04-19_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-06-14_21:02:30___2019-06-14_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-01-19_22:12:29___2019-01-19_22:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-11-14_22:11:50___2019-11-14_22:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-01-01_22:59:59___2019-01-01_21:59:07.csv'),\n",
       " Path('../data/comments/by_date/2019-12-19_22:09:07___2019-12-19_22:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-12-01_22:59:59___2019-12-01_22:36:30.csv'),\n",
       " Path('../data/comments/by_date/2019-04-01_21:59:59___2019-04-01_21:05:49.csv'),\n",
       " Path('../data/comments/by_date/2019-11-01_22:59:59___2019-11-01_22:06:28.csv'),\n",
       " Path('../data/comments/by_date/2019-10-01_21:59:59___2019-10-01_21:14:05.csv'),\n",
       " Path('../data/comments/by_date/2019-09-19_21:12:20___2019-09-19_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-01-07_22:14:11___2019-01-07_22:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-04-14_21:01:38___2019-04-14_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-07-19_21:10:09___2019-07-19_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-05-01_21:59:59___2019-05-01_21:10:03.csv'),\n",
       " Path('../data/comments/by_date/2019-09-14_21:17:45___2019-09-14_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-03-19_22:08:57___2019-03-19_22:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-09-01_21:59:59___2019-09-01_21:03:54.csv'),\n",
       " Path('../data/comments/by_date/2019-01-14_22:04:48___2019-01-14_22:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-09-07_21:09:47___2019-09-07_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-10-14_21:33:09___2019-10-14_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-08-19_21:10:07___2019-08-19_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-10-19_21:04:06___2019-10-19_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-12-14_22:34:00___2019-12-14_22:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-03-07_22:06:55___2019-03-07_22:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-02-19_22:06:50___2019-02-19_22:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-08-14_21:13:49___2019-08-14_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-05-19_21:16:29___2019-05-19_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-07-07_21:07:10___2019-07-07_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-04-07_21:10:44___2019-04-07_21:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-11-19_22:15:40___2019-11-19_22:59:59.csv'),\n",
       " Path('../data/comments/by_date/2019-06-19_21:09:12___2019-06-19_21:59:59.csv')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_comments_paths_year(COMMENTS_DIR, '2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(get_comments_paths_year(COMMENTS_DIR, '2019')) == 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(get_comments_paths_year(COMMENTS_DIR, '2020')) == 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_paths_year = get_comments_paths_year(COMMENTS_DIR, YEAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENTS_DIR_SUBR = '../data/comments/subr/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBR = 'conspiracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_comments_paths_subr(COMMENTS_DIR_SUBR, SUBR):\n",
    "\tcomments_subr_dir_path = Path(COMMENTS_DIR_SUBR)\n",
    "\tcomments_subr_paths = list(comments_subr_dir_path.glob(f'{SUBR}*.csv'))\n",
    "\treturn comments_subr_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_paths_subr = get_comments_paths_subr(COMMENTS_DIR_SUBR, SUBR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read `1` comments `csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = comment_paths_year[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def read_comm_csv(fpath):\n",
    "    try:\n",
    "        # removed because new method for writing retrieved data out already does date conversion beforehand\n",
    "        # date_parser = lambda x: pd.to_datetime(x, unit='s', errors='coerce')\n",
    "        comments = pd.read_csv(\n",
    "            fpath,\n",
    "            usecols=['id', 'created_utc', 'author', 'subreddit', 'body'],\n",
    "            dtype={\n",
    "                'id': 'string',\n",
    "                # 'created_utc': int, s. above\n",
    "                'author': 'string',\n",
    "                'subreddit': 'string',\n",
    "                'body': 'string'\n",
    "            },\n",
    "            parse_dates=['created_utc'],\n",
    "            # date_parser=date_parser,\n",
    "            low_memory=False,\n",
    "            lineterminator='\\n'\n",
    "        )\n",
    "        comments_clean = comments\\\n",
    "            .dropna()\\\n",
    "            .drop_duplicates(subset='id')\n",
    "        return comments_clean\n",
    "    except FileNotFoundError:\n",
    "        print(f'{fpath} not found on disk')\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f'{fpath} is empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = read_comm_csv(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200000 entries, 0 to 199999\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   author       200000 non-null  string        \n",
      " 1   body         200000 non-null  string        \n",
      " 2   created_utc  200000 non-null  datetime64[ns]\n",
      " 3   id           200000 non-null  string        \n",
      " 4   subreddit    200000 non-null  string        \n",
      "dtypes: datetime64[ns](1), string(4)\n",
      "memory usage: 9.2 MB\n"
     ]
    }
   ],
   "source": [
    "comments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read multiple comment `csv` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def read_comm_csvs(fpaths: list):\n",
    "    comments_lst = []\n",
    "    for fpath in fpaths:\n",
    "        comments = read_comm_csv(fpath)\n",
    "        comments_lst.append(comments)\n",
    "    comments_concat = pd.concat(\n",
    "        comments_lst,\n",
    "        axis=0,\n",
    "        ignore_index=True\n",
    "    )\n",
    "    return comments_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = read_comm_csvs(comment_paths_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit\n",
       "AskReddit             429516\n",
       "politics              146023\n",
       "memes                  99027\n",
       "teenagers              89685\n",
       "dankmemes              84107\n",
       "                       ...  \n",
       "no_u                       1\n",
       "CuteBobby                  1\n",
       "no_drama                   1\n",
       "WorldBoxGodSandbox         1\n",
       "FatFurryPorn               1\n",
       "Length: 66885, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.value_counts('subreddit')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
