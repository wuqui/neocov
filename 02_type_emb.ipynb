{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp type_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    ['A', 'test', 'sentence'],\n",
    "    ['Another', 'test', 'sentence']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Corpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self, docs):\n",
    "        self.docs_clean = docs\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc in self.docs_clean:\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train_model(corpus,\n",
    "              MIN_COUNT=5,\n",
    "              SIZE=300,\n",
    "              WORKERS=8,\n",
    "              WINDOW=5,\n",
    "              EPOCHS=5\n",
    "              ):\n",
    "    model = Word2Vec(\n",
    "        corpus,\n",
    "        min_count=MIN_COUNT,\n",
    "        vector_size=SIZE,\n",
    "        workers=WORKERS,\n",
    "        window=WINDOW,\n",
    "        epochs=EPOCHS\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train_model(corpus,\n",
    "              MIN_COUNT=5,\n",
    "              SIZE=300,\n",
    "              WORKERS=8,\n",
    "              WINDOW=5):\n",
    "    model = Word2Vec(\n",
    "        corpus,\n",
    "        min_count=MIN_COUNT,\n",
    "        vector_size=SIZE,\n",
    "        workers=WORKERS,\n",
    "        window=WINDOW\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(corpus, MIN_COUNT=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Coronavirus', 'conspiracy']\n",
    "# model_names = ['Coronavirus', 'LockdownSkepticism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [dict() for name in model_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "\tmodel['name'] = model_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "\tmodel['path'] = f'../out/models/{model[\"name\"]}.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "for model in models:\n",
    "\tmodel['model'] = Word2Vec.load(model['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## align models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = train_model(corpus=[['The', 'bank', 'of', 'the', 'river']], MIN_COUNT=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = train_model(corpus=[['The', 'bank', 'of', 'England']], MIN_COUNT=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model_1.wv.key_to_index) != len(model_2.wv.vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def intersection_align_gensim(m1, m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.index_to_key)\n",
    "    vocab_m2 = set(m2.wv.index_to_key)\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1 & vocab_m2\n",
    "    if words: common_vocab &= set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n",
    "    # print(len(common_vocab))\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1, m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.key_to_index[w] for w in common_vocab]\n",
    "        old_arr = m.wv.vectors\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.vectors = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        new_key_to_index = {}\n",
    "        new_index_to_key = []\n",
    "        for new_index, key in enumerate(common_vocab):\n",
    "            new_key_to_index[key] = new_index\n",
    "            new_index_to_key.append(key)\n",
    "        m.wv.key_to_index = new_key_to_index\n",
    "        m.wv.index_to_key = new_index_to_key\n",
    "        \n",
    "        print(len(m.wv.key_to_index), len(m.wv.vectors))\n",
    "        \n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"\n",
    "    Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf\n",
    "    Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "        \n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the (normalized) embedding matrices\n",
    "    base_vecs = in_base_embed.wv.get_normed_vectors()\n",
    "    other_vecs = in_other_embed.wv.get_normed_vectors()\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one, i.e. multiplying the embedding matrix by \"ortho\"\n",
    "    other_embed.wv.vectors = (other_embed.wv.vectors).dot(ortho)    \n",
    "    \n",
    "    return other_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_procrustes_align_gensim(model_1, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model_1.wv.key_to_index) == len(model_2.wv.vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## measure distances between types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def measure_distances(model_1, model_2):\n",
    "    distances = pd.DataFrame(\n",
    "        columns=('lex', 'dist_sem', \"freq_1\", \"freq_2\"),\n",
    "        data=(\n",
    "            #[w, spatial.distance.euclidean(model_1.wv[w], model_2.wv[w]),\n",
    "            #[w, np.sum(model_1.wv[w] * model_2.wv[w]) / (np.linalg.norm(model_1.wv[w]) * np.linalg.norm(model_2.wv[w])),\n",
    "            [w, spatial.distance.cosine(model_1.wv[w], model_2.wv[w]),\n",
    "             model_1.wv.get_vecattr(w, \"count\"),\n",
    "             model_2.wv.get_vecattr(w, \"count\")\n",
    "             ] for w in model_1.wv.index_to_key\n",
    "        )\n",
    "    )\n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = measure_distances(model_1, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances\\\n",
    "    .sort_values('dist_sem', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get nearest neighbours of lexemes for 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_nearest_neighbours_models(lex, freq_min, model_1, model_2, topn=100_000, k=10):\n",
    "    nbs = []\n",
    "    for count, model in enumerate([model_1, model_2]):\n",
    "        for nb, dist in model.wv.most_similar(lex, topn=topn):\n",
    "            if model.wv.get_vecattr(nb, 'count') > freq_min:\n",
    "                d = {}\n",
    "                d['model'] = count + 1\n",
    "                d['lex'] = nb\n",
    "                d['similarity'] = dist\n",
    "                d['freq'] = model.wv.get_vecattr(nb, \"count\")\n",
    "                nbs.append(d)\n",
    "    nbs_df = pd.DataFrame(nbs)\n",
    "    nbs_df = nbs_df\\\n",
    "        .query('freq > @freq_min')\\\n",
    "        .groupby('model', group_keys=False)\\\n",
    "        .apply(lambda group: group.nlargest(k, 'similarity'))\n",
    "    nbs_model_1 = nbs_df.query('model == 1')\n",
    "    nbs_model_2 = nbs_df.query('model == 2')\n",
    "    return nbs_model_1, nbs_model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## project embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "smart_procrustes_align_gensim(models[0]['model'], models[1]['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_pole_avg(model, lex: str, k=10):\n",
    "\tvecs = []\n",
    "\tvecs.append(model.wv[lex])\n",
    "\tfor closest_word, similarity in model.wv.most_similar(positive=lex, topn=k):\n",
    "\t\tvecs.append(model.wv[closest_word])\n",
    "\tpole_avg = np.mean(vecs, axis=0)\n",
    "\treturn pole_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_sem_axis_avg(model, pole_word_1: str, pole_word_2: str, k=10):\n",
    "\tpole_1_avg = get_pole_avg(model, pole_word_1, k)\n",
    "\tpole_2_avg = get_pole_avg(model, pole_word_2, k)\n",
    "\tsem_axis = pole_1_avg - pole_2_avg\n",
    "\treturn sem_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_axis_sim(lex: str, pole_word_1: str, pole_word_2: str, model, k=10):\n",
    "\tsem_axis = make_sem_axis_avg(model, pole_word_1, pole_word_2, k)\n",
    "\tlex_vec = model.wv.get_vector(lex)\n",
    "\tsim_cos = 1 - spatial.distance.cosine(lex_vec, sem_axis)\n",
    "\treturn sim_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_axis_sims(lexs: list, models, pole_words: list, k=10):\n",
    "\tsims = []\n",
    "\tfor lex in lexs:\n",
    "\t\tfor model in models:\n",
    "\t\t\tsim = {}\n",
    "\t\t\tsim['subreddit'] = model['name']\n",
    "\t\t\tsim['lex'] = lex\n",
    "\t\t\tsim['sim'] = get_axis_sim(lex, pole_words[0], pole_words[1], model['model'], k)\n",
    "\t\t\tsims.append(sim)\n",
    "\tsims_df = pd.DataFrame(sims)\n",
    "\treturn sims_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pole_words = ['good', 'bad']\n",
    "# pole_words = ['objective', 'subjective']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexs = [\n",
    "\t'regulations', 'politics',\n",
    "\t'government', 'mandate', \n",
    "\t'science', 'research',\n",
    "\t'shutdown', 'shutdowns', \n",
    "\t'lockdown', 'lockdowns', \n",
    "\t'vaccine', 'vaccines', \n",
    "\t'mask', 'masks',\n",
    "\t# 'tree', 'food', 'drink', 'air', 'sun'\n",
    "\t# 'yellow', 'purple', 'orange' \n",
    "\t# 'give', 'take',\n",
    "\t# 'you', 'i', 'the', 'of',\n",
    "\t# 'good', 'bad'\n",
    "\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "proj_sims = get_axis_sims(lexs, models, pole_words, k=10)\n",
    "\n",
    "proj_sims_chart = alt.Chart(proj_sims).mark_line(point=True).encode(\n",
    "\tx='sim',\n",
    "\ty=alt.Y('lex', sort=None),\n",
    "\tcolor='subreddit'\n",
    ")\n",
    "\n",
    "proj_sims_chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit ('neocov': conda)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
