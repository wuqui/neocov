{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neocov.read_data import *\n",
    "from neocov.preproc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeoCov\n",
    "\n",
    "> Semantic change and social semantic variation of Covid-related English neologisms on Reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/'\n",
    "COMMENTS_DIAC_DIR = f'{DATA_DIR}comments/by_date/'\n",
    "OUT_DIR = '..out/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = '2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_paths_year = get_comments_paths_year(COMMENTS_DIAC_DIR, YEAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "comments = read_comm_csvs(comment_paths_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.value_counts('subreddit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "comments = clean_comments(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-driving",
   "metadata": {},
   "source": [
    "### Create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self, docs_clean):\n",
    "        self.docs_clean = docs_clean\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc in self.docs_clean:\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(comments['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_emb(corpus, \n",
    "              MIN_COUNT=5, \n",
    "              SIZE=300, \n",
    "              WORKERS=8, \n",
    "              WINDOW=5):\n",
    "    model = Word2Vec(\n",
    "        corpus, \n",
    "        min_count=MIN_COUNT,\n",
    "        vector_size=SIZE,\n",
    "        workers=WORKERS, \n",
    "        window=WINDOW\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = train_emb(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-flash",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'{OUT_DIR}models/{YEAR}.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-necessity",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2019 = Word2Vec.load('{OUT_DIR}models/2019.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2020 = Word2Vec.load('{OUT_DIR}models/2020.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-blank",
   "metadata": {},
   "source": [
    "## Align models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_align_gensim(m1, m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.index_to_key)\n",
    "    vocab_m2 = set(m2.wv.index_to_key)\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1 & vocab_m2\n",
    "    if words: common_vocab &= set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n",
    "    # print(len(common_vocab))\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1, m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.key_to_index[w] for w in common_vocab]\n",
    "        old_arr = m.wv.vectors\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.vectors = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        new_key_to_index = {}\n",
    "        new_index_to_key = []\n",
    "        for new_index, key in enumerate(common_vocab):\n",
    "            new_key_to_index[key] = new_index\n",
    "            new_index_to_key.append(key)\n",
    "        m.wv.key_to_index = new_key_to_index\n",
    "        m.wv.index_to_key = new_index_to_key\n",
    "        \n",
    "        print(len(m.wv.key_to_index), len(m.wv.vectors))\n",
    "        \n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"\n",
    "    Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf\n",
    "    Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "        \n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the (normalized) embedding matrices\n",
    "    base_vecs = in_base_embed.wv.get_normed_vectors()\n",
    "    other_vecs = in_other_embed.wv.get_normed_vectors()\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one, i.e. multiplying the embedding matrix by \"ortho\"\n",
    "    other_embed.wv.vectors = (other_embed.wv.vectors).dot(ortho)    \n",
    "    \n",
    "    return other_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-awareness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190756 190756\n",
      "190756 190756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x141570640>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smart_procrustes_align_gensim(model_2019, model_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-cutting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>190756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>190756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intersection</td>\n",
       "      <td>190756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  words\n",
       "0          2019  190756\n",
       "1          2020  190756\n",
       "2  intersection  190756"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_vocab = pd.DataFrame(\n",
    "    data=[\n",
    "        ['2019', len(model_2019.wv.key_to_index)],\n",
    "        ['2020', len(model_2020.wv.key_to_index)],\n",
    "        ['intersection', len(set(model_2019.wv.key_to_index).intersection(set(model_2020.wv.key_to_index)))]\n",
    "    ],\n",
    "    columns=['', 'words']\n",
    ")\n",
    "\n",
    "models_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_vocab.to_csv(f'{OUT_DIR}models_vocab.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-lighting",
   "metadata": {},
   "source": [
    "## Measure distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_distances(model_1, model_2):\n",
    "    distances = pd.DataFrame(\n",
    "    data=(\n",
    "            #[w, spatial.distance.euclidean(model_1.wv[w], model_2.wv[w]), \n",
    "            #[w, np.sum(model_1.wv[w] * model_2.wv[w]) / (np.linalg.norm(model_1.wv[w]) * np.linalg.norm(model_2.wv[w])), \n",
    "            [w, spatial.distance.cosine(model_1.wv[w], model_2.wv[w]), \n",
    "             model_1.wv.get_vecattr(w, \"count\"), \n",
    "             model_2.wv.get_vecattr(w, \"count\")\n",
    "            ] for w in model_1.wv.index_to_key\n",
    "        ), \n",
    "        columns = ('lex', 'dist_sem', \"freq_1\", \"freq_2\")\n",
    "    )\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = measure_distances(model_2019, model_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lex</th>\n",
       "      <th>dist_sem</th>\n",
       "      <th>freq_1</th>\n",
       "      <th>freq_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181299</th>\n",
       "      <td>financiados</td>\n",
       "      <td>1.270406</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165232</th>\n",
       "      <td>______________________________________________...</td>\n",
       "      <td>1.257892</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181454</th>\n",
       "      <td>2ffireemblem</td>\n",
       "      <td>1.247719</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189647</th>\n",
       "      <td>obedece</td>\n",
       "      <td>1.239514</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126402</th>\n",
       "      <td>1281</td>\n",
       "      <td>1.218590</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>years</td>\n",
       "      <td>0.027202</td>\n",
       "      <td>175105</td>\n",
       "      <td>192696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171086</th>\n",
       "      <td>ppx_yo_dt_b_asin_title_o09_s00</td>\n",
       "      <td>0.025620</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46607</th>\n",
       "      <td>imagestabilization</td>\n",
       "      <td>0.025614</td>\n",
       "      <td>85</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144119</th>\n",
       "      <td>ppx_yo_dt_b_asin_title_o03_s00</td>\n",
       "      <td>0.018814</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68529</th>\n",
       "      <td>u5e9htr</td>\n",
       "      <td>0.012333</td>\n",
       "      <td>42</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190756 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      lex  dist_sem  freq_1  \\\n",
       "181299                                        financiados  1.270406       8   \n",
       "165232  ______________________________________________...  1.257892       9   \n",
       "181454                                       2ffireemblem  1.247719       8   \n",
       "189647                                            obedece  1.239514       7   \n",
       "126402                                               1281  1.218590      14   \n",
       "...                                                   ...       ...     ...   \n",
       "175                                                 years  0.027202  175105   \n",
       "171086                     ppx_yo_dt_b_asin_title_o09_s00  0.025620       8   \n",
       "46607                                  imagestabilization  0.025614      85   \n",
       "144119                     ppx_yo_dt_b_asin_title_o03_s00  0.018814      11   \n",
       "68529                                             u5e9htr  0.012333      42   \n",
       "\n",
       "        freq_2  \n",
       "181299       9  \n",
       "165232      10  \n",
       "181454       9  \n",
       "189647       8  \n",
       "126402      16  \n",
       "...        ...  \n",
       "175     192696  \n",
       "171086       9  \n",
       "46607       92  \n",
       "144119      13  \n",
       "68529       46  \n",
       "\n",
       "[190756 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances\\\n",
    "    .sort_values('dist_sem', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sem_change_cands(distances, k=20, freq_min=1):\n",
    "    sem_change_cands = (distances\n",
    "        .query('freq_1 > @freq_min and freq_2 > @freq_min')\n",
    "        .query('lex.str.isalpha() == True')\n",
    "        .query('lex.str.len() > 3')\n",
    "        .nlargest(k, 'dist_sem')\n",
    "        .reset_index(drop=True)\n",
    "        )\n",
    "    return sem_change_cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-huntington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lex</th>\n",
       "      <th>dist_sem</th>\n",
       "      <th>freq_1</th>\n",
       "      <th>freq_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>corona</td>\n",
       "      <td>0.927504</td>\n",
       "      <td>3553</td>\n",
       "      <td>3684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pandemic</td>\n",
       "      <td>0.912615</td>\n",
       "      <td>9504</td>\n",
       "      <td>9957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>snapchatting</td>\n",
       "      <td>0.912304</td>\n",
       "      <td>2262</td>\n",
       "      <td>2345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dodo</td>\n",
       "      <td>0.864197</td>\n",
       "      <td>1651</td>\n",
       "      <td>1716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rubric</td>\n",
       "      <td>0.839424</td>\n",
       "      <td>1058</td>\n",
       "      <td>1109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>neon</td>\n",
       "      <td>0.393886</td>\n",
       "      <td>1326</td>\n",
       "      <td>1391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>villagers</td>\n",
       "      <td>0.393821</td>\n",
       "      <td>1274</td>\n",
       "      <td>1333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>goose</td>\n",
       "      <td>0.391982</td>\n",
       "      <td>1197</td>\n",
       "      <td>1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>mute</td>\n",
       "      <td>0.391320</td>\n",
       "      <td>5323</td>\n",
       "      <td>5505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>lastly</td>\n",
       "      <td>0.389894</td>\n",
       "      <td>2129</td>\n",
       "      <td>2193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             lex  dist_sem  freq_1  freq_2\n",
       "0         corona  0.927504    3553    3684\n",
       "1       pandemic  0.912615    9504    9957\n",
       "2   snapchatting  0.912304    2262    2345\n",
       "3           dodo  0.864197    1651    1716\n",
       "4         rubric  0.839424    1058    1109\n",
       "..           ...       ...     ...     ...\n",
       "95          neon  0.393886    1326    1391\n",
       "96     villagers  0.393821    1274    1333\n",
       "97         goose  0.391982    1197    1260\n",
       "98          mute  0.391320    5323    5505\n",
       "99        lastly  0.389894    2129    2193\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem_change_cands = get_sem_change_cands(distances, k=100, freq_min=1000)\n",
    "sem_change_cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-meter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexeme</th>\n",
       "      <th>SemDist</th>\n",
       "      <th>freq_1</th>\n",
       "      <th>freq_2</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>corona</td>\n",
       "      <td>0.93</td>\n",
       "      <td>3553</td>\n",
       "      <td>3684</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pandemic</td>\n",
       "      <td>0.91</td>\n",
       "      <td>9504</td>\n",
       "      <td>9957</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>snapchatting</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2262</td>\n",
       "      <td>2345</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dodo</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1651</td>\n",
       "      <td>1716</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rubric</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1058</td>\n",
       "      <td>1109</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nices</td>\n",
       "      <td>0.81</td>\n",
       "      <td>7457</td>\n",
       "      <td>7710</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyphens</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1044</td>\n",
       "      <td>1096</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>asterisks</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1085</td>\n",
       "      <td>1138</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>distancing</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2910</td>\n",
       "      <td>3038</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>newbies</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1566</td>\n",
       "      <td>1644</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>delet</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1269</td>\n",
       "      <td>1329</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>blah</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3683</td>\n",
       "      <td>3826</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tracing</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1228</td>\n",
       "      <td>1293</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>warzone</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1022</td>\n",
       "      <td>1070</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>yandex</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1111</td>\n",
       "      <td>1169</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gaysnapchat</td>\n",
       "      <td>0.70</td>\n",
       "      <td>5145</td>\n",
       "      <td>5328</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>lockdown</td>\n",
       "      <td>0.70</td>\n",
       "      <td>4494</td>\n",
       "      <td>4642</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>muda</td>\n",
       "      <td>0.69</td>\n",
       "      <td>3300</td>\n",
       "      <td>3436</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>specifying</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1135</td>\n",
       "      <td>1191</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>origi</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1219</td>\n",
       "      <td>1282</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Lexeme SemDist  freq_1  freq_2    \n",
       "0         corona    0.93    3553    3684   1\n",
       "1       pandemic    0.91    9504    9957   2\n",
       "2   snapchatting    0.91    2262    2345   3\n",
       "3           dodo    0.86    1651    1716   4\n",
       "4         rubric    0.84    1058    1109   5\n",
       "5          nices    0.81    7457    7710   6\n",
       "6        hyphens    0.81    1044    1096   7\n",
       "7      asterisks    0.81    1085    1138   8\n",
       "8     distancing    0.79    2910    3038   9\n",
       "9        newbies    0.78    1566    1644  10\n",
       "10         delet    0.78    1269    1329  11\n",
       "11          blah    0.75    3683    3826  12\n",
       "12       tracing    0.75    1228    1293  13\n",
       "13       warzone    0.71    1022    1070  14\n",
       "14        yandex    0.70    1111    1169  15\n",
       "15   gaysnapchat    0.70    5145    5328  16\n",
       "16      lockdown    0.70    4494    4642  17\n",
       "17          muda    0.69    3300    3436  18\n",
       "18    specifying    0.68    1135    1191  19\n",
       "19         origi    0.67    1219    1282  20"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem_change_cands_out = sem_change_cands\\\n",
    "    .nlargest(100, 'dist_sem')\\\n",
    "    .assign(index_1 = lambda df: df.index + 1)\\\n",
    "    .assign(dist_sem = lambda df: df['dist_sem'].round(2))\\\n",
    "    .assign(dist_sem = lambda df: df['dist_sem'].apply('{:.2f}'.format))\\\n",
    "    .rename({'index_1': '', 'lex': 'Lexeme', 'dist_sem': 'SemDist'}, axis=1)\n",
    "\n",
    "sem_change_cands_out.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_change_cands_out.to_csv(\n",
    "        '{OUT_DIR}sem_change_cands.csv',\n",
    "        columns=['', 'Lexeme', 'SemDist'],\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-tomato",
   "metadata": {},
   "source": [
    "## Inspect nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEX_NBS = 'distancing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_neighbours_models(lex, freq_min, model_1, model_2, topn=100_000):\n",
    "    nbs = []\n",
    "    for count, model in enumerate([model_1, model_2]):\n",
    "        for nb, dist in model.wv.most_similar(lex, topn=topn):\n",
    "            if model.wv.get_vecattr(nb, 'count') > freq_min:\n",
    "                d = {}\n",
    "                d['model'] = count + 1\n",
    "                d['lex'] = nb\n",
    "                d['similarity'] = dist\n",
    "                d['freq'] = model.wv.get_vecattr(nb, \"count\")\n",
    "                nbs.append(d)\n",
    "    nbs_df = pd.DataFrame(nbs)\n",
    "    nbs_df = nbs_df\\\n",
    "        .query('freq > @freq_min')\\\n",
    "        .groupby('model', group_keys=False)\\\n",
    "        .apply(lambda group: group.nlargest(10, 'similarity'))\n",
    "    nbs_model_1 = nbs_df.query('model == 1')\n",
    "    nbs_model_2 = nbs_df.query('model == 2')\n",
    "    return nbs_model_1, nbs_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-majority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>lex</th>\n",
       "      <th>similarity</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>distanced</td>\n",
       "      <td>0.837815</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>disassociate</td>\n",
       "      <td>0.717895</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>detaching</td>\n",
       "      <td>0.685801</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>deluding</td>\n",
       "      <td>0.667654</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>bettering</td>\n",
       "      <td>0.633784</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>incriminate</td>\n",
       "      <td>0.629239</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>isolating</td>\n",
       "      <td>0.629057</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>distract</td>\n",
       "      <td>0.617911</td>\n",
       "      <td>1553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>handicapping</td>\n",
       "      <td>0.610600</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>detach</td>\n",
       "      <td>0.603991</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model           lex  similarity  freq\n",
       "0      1     distanced    0.837815   309\n",
       "1      1  disassociate    0.717895    93\n",
       "2      1     detaching    0.685801    61\n",
       "3      1      deluding    0.667654   104\n",
       "4      1     bettering    0.633784   198\n",
       "5      1   incriminate    0.629239    80\n",
       "6      1     isolating    0.629057   685\n",
       "7      1      distract    0.617911  1553\n",
       "8      1  handicapping    0.610600    54\n",
       "9      1        detach    0.603991   244"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>lex</th>\n",
       "      <th>similarity</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36688</th>\n",
       "      <td>2</td>\n",
       "      <td>distanced</td>\n",
       "      <td>0.553989</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36689</th>\n",
       "      <td>2</td>\n",
       "      <td>isolation</td>\n",
       "      <td>0.547227</td>\n",
       "      <td>2037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36690</th>\n",
       "      <td>2</td>\n",
       "      <td>gatherings</td>\n",
       "      <td>0.519332</td>\n",
       "      <td>921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36691</th>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.511493</td>\n",
       "      <td>11355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36692</th>\n",
       "      <td>2</td>\n",
       "      <td>lockdowns</td>\n",
       "      <td>0.499619</td>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36693</th>\n",
       "      <td>2</td>\n",
       "      <td>quarantines</td>\n",
       "      <td>0.487039</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36694</th>\n",
       "      <td>2</td>\n",
       "      <td>lockdown</td>\n",
       "      <td>0.483064</td>\n",
       "      <td>4642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36695</th>\n",
       "      <td>2</td>\n",
       "      <td>masks</td>\n",
       "      <td>0.477628</td>\n",
       "      <td>8997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36696</th>\n",
       "      <td>2</td>\n",
       "      <td>precautions</td>\n",
       "      <td>0.469785</td>\n",
       "      <td>1237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36697</th>\n",
       "      <td>2</td>\n",
       "      <td>quarantine</td>\n",
       "      <td>0.468756</td>\n",
       "      <td>5225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model          lex  similarity   freq\n",
       "36688      2    distanced    0.553989    326\n",
       "36689      2    isolation    0.547227   2037\n",
       "36690      2   gatherings    0.519332    921\n",
       "36691      2     distance    0.511493  11355\n",
       "36692      2    lockdowns    0.499619    991\n",
       "36693      2  quarantines    0.487039    159\n",
       "36694      2     lockdown    0.483064   4642\n",
       "36695      2        masks    0.477628   8997\n",
       "36696      2  precautions    0.469785   1237\n",
       "36697      2   quarantine    0.468756   5225"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbs_model_1, nbs_model_2 = get_nearest_neighbours_models(\n",
    "    lex=LEX_NBS, \n",
    "    freq_min=50,\n",
    "    model_1=model_2019, \n",
    "    model_2=model_2020\n",
    ")\n",
    "\n",
    "display(\n",
    "    nbs_model_1,\n",
    "    nbs_model_2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbs_model_1.to_csv(f'{OUT_DIR}neighbours/{LEX_NBS}_2019.csv')\n",
    "nbs_model_2.to_csv(f'{OUT_DIR}neighbours/{LEX_NBS}_2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-chorus",
   "metadata": {},
   "source": [
    "# Inspect subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-coordinator",
   "metadata": {},
   "source": [
    "## read comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_paths = get_comments_paths_year(COMMENTS_DIAC_DIR, YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-passing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.7 s, sys: 6.22 s, total: 54 s\n",
      "Wall time: 54.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avinse</td>\n",
       "      <td>Username Checks Out</td>\n",
       "      <td>2019-05-07 21:11:36</td>\n",
       "      <td>emrv0h9</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KeepingDankMemesDank</td>\n",
       "      <td>If this is a dank meme, **Upvote** this commen...</td>\n",
       "      <td>2019-05-07 21:11:37</td>\n",
       "      <td>emrv0jp</td>\n",
       "      <td>dankmemes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UhPhrasing</td>\n",
       "      <td>Just threaten them that you'll call the corpor...</td>\n",
       "      <td>2019-05-07 21:11:37</td>\n",
       "      <td>emrv0jq</td>\n",
       "      <td>golf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>2019-05-07 21:11:37</td>\n",
       "      <td>emrv0jr</td>\n",
       "      <td>Barca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EnergetikNA</td>\n",
       "      <td>honestly, do you really wanna go through an en...</td>\n",
       "      <td>2019-05-07 21:11:37</td>\n",
       "      <td>emrv0js</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9599974</th>\n",
       "      <td>DogBeersHadOne</td>\n",
       "      <td>Guy who made the crossbuck had one job. One go...</td>\n",
       "      <td>2019-06-19 21:59:59</td>\n",
       "      <td>erl9mvx</td>\n",
       "      <td>trains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9599975</th>\n",
       "      <td>VenomousCoffee</td>\n",
       "      <td>Page number? Picture of the page?</td>\n",
       "      <td>2019-06-19 21:59:59</td>\n",
       "      <td>erl9mvw</td>\n",
       "      <td>marvelstudios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9599976</th>\n",
       "      <td>Homerundude698</td>\n",
       "      <td>So sexy baby</td>\n",
       "      <td>2019-06-19 21:59:59</td>\n",
       "      <td>erl9mvv</td>\n",
       "      <td>gonewild30plus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9599977</th>\n",
       "      <td>CircusRama</td>\n",
       "      <td>Removed for Rule 8</td>\n",
       "      <td>2019-06-19 21:59:59</td>\n",
       "      <td>erl9mwa</td>\n",
       "      <td>fivenightsatfreddys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9599978</th>\n",
       "      <td>BusShelter</td>\n",
       "      <td>This is barely even half an inch by the looks ...</td>\n",
       "      <td>2019-06-19 21:59:59</td>\n",
       "      <td>erl9muw</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9599979 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       author  \\\n",
       "0                      Avinse   \n",
       "1        KeepingDankMemesDank   \n",
       "2                  UhPhrasing   \n",
       "3                   [deleted]   \n",
       "4                 EnergetikNA   \n",
       "...                       ...   \n",
       "9599974        DogBeersHadOne   \n",
       "9599975        VenomousCoffee   \n",
       "9599976        Homerundude698   \n",
       "9599977            CircusRama   \n",
       "9599978            BusShelter   \n",
       "\n",
       "                                                      body  \\\n",
       "0                                      Username Checks Out   \n",
       "1        If this is a dank meme, **Upvote** this commen...   \n",
       "2        Just threaten them that you'll call the corpor...   \n",
       "3                                                [removed]   \n",
       "4        honestly, do you really wanna go through an en...   \n",
       "...                                                    ...   \n",
       "9599974  Guy who made the crossbuck had one job. One go...   \n",
       "9599975                  Page number? Picture of the page?   \n",
       "9599976                                       So sexy baby   \n",
       "9599977                                 Removed for Rule 8   \n",
       "9599978  This is barely even half an inch by the looks ...   \n",
       "\n",
       "                created_utc       id            subreddit  \n",
       "0       2019-05-07 21:11:36  emrv0h9            AskReddit  \n",
       "1       2019-05-07 21:11:37  emrv0jp            dankmemes  \n",
       "2       2019-05-07 21:11:37  emrv0jq                 golf  \n",
       "3       2019-05-07 21:11:37  emrv0jr                Barca  \n",
       "4       2019-05-07 21:11:37  emrv0js               soccer  \n",
       "...                     ...      ...                  ...  \n",
       "9599974 2019-06-19 21:59:59  erl9mvx               trains  \n",
       "9599975 2019-06-19 21:59:59  erl9mvw        marvelstudios  \n",
       "9599976 2019-06-19 21:59:59  erl9mvv       gonewild30plus  \n",
       "9599977 2019-06-19 21:59:59  erl9mwa  fivenightsatfreddys  \n",
       "9599978 2019-06-19 21:59:59  erl9muw               soccer  \n",
       "\n",
       "[9599979 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "comments = read_comm_csvs(comments_paths)\n",
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: filter comments\n",
    "\n",
    "- [ ] remove duplicates\n",
    "- [ ] remove bots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-rocket",
   "metadata": {},
   "source": [
    "## get subreddit counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subr_counts(comments):\n",
    "    subr_counts = comments\\\n",
    "        .groupby('subreddit')\\\n",
    "        .agg(comments_num = ('subreddit', 'count'))\\\n",
    "        .sort_values('comments_num', ascending=False)\n",
    "    return subr_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "subr_counts = get_subr_counts(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subr_counts(subr_counts, k=20):\n",
    "    chart = subr_counts\\\n",
    "        .reset_index()\\\n",
    "        .iloc[:k]\\\n",
    "        .pipe(alt.Chart)\\\n",
    "            .mark_bar()\\\n",
    "            .encode(\n",
    "                x=alt.X('comments_num:Q'),\n",
    "                y=alt.Y('subreddit:N', sort='-x')\n",
    "            )\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-spider",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-1d981c61e4bd4d54a9b18eeeaf7cd7ec\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-1d981c61e4bd4d54a9b18eeeaf7cd7ec\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-1d981c61e4bd4d54a9b18eeeaf7cd7ec\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-151658c588b77c0f8397fe41b7b06449\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"field\": \"comments_num\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"subreddit\", \"sort\": \"-x\", \"type\": \"nominal\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-151658c588b77c0f8397fe41b7b06449\": [{\"subreddit\": \"AskReddit\", \"comments_num\": 429516}, {\"subreddit\": \"politics\", \"comments_num\": 146023}, {\"subreddit\": \"memes\", \"comments_num\": 99027}, {\"subreddit\": \"teenagers\", \"comments_num\": 89685}, {\"subreddit\": \"dankmemes\", \"comments_num\": 84107}, {\"subreddit\": \"PatriotsvsTexansLivz\", \"comments_num\": 80476}, {\"subreddit\": \"soccer\", \"comments_num\": 73348}, {\"subreddit\": \"nba\", \"comments_num\": 69130}, {\"subreddit\": \"AmItheAsshole\", \"comments_num\": 65271}, {\"subreddit\": \"funny\", \"comments_num\": 64256}, {\"subreddit\": \"NFLWeek14Lives\", \"comments_num\": 60055}, {\"subreddit\": \"nfl\", \"comments_num\": 58602}, {\"subreddit\": \"unpopularopinion\", \"comments_num\": 55227}, {\"subreddit\": \"worldnews\", \"comments_num\": 54755}, {\"subreddit\": \"ufc245fightonline\", \"comments_num\": 48229}, {\"subreddit\": \"The_Donald\", \"comments_num\": 46733}, {\"subreddit\": \"gaming\", \"comments_num\": 45341}, {\"subreddit\": \"UFC245StreamToday\", \"comments_num\": 45162}, {\"subreddit\": \"CFB\", \"comments_num\": 44439}, {\"subreddit\": \"news\", \"comments_num\": 43733}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subr_counts_plt = plot_subr_counts(subr_counts, k=20)\n",
    "subr_counts_plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "subr_counts_fname = 'Covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "subr_counts_plt.save(f'out/subr_counts_plt_{subr_counts_fname}.svg', scale_factor=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments\\\n",
    "    .query('subreddit == \"hdsportsfeedtv\"')\\\n",
    "     .sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
