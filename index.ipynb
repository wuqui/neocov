{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neocov.read_data import *\n",
    "from neocov.preproc import *\n",
    "from neocov.type_emb import *\n",
    "from neocov.communities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/'\n",
    "COMMENTS_DIAC_DIR = f'{DATA_DIR}comments/by_date/'\n",
    "OUT_DIR = '../out/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeoCov\n",
    "\n",
    "> Semantic change and social semantic variation of Covid-related English neologisms on Reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = '2020'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_paths_year = get_comments_paths_year(COMMENTS_DIAC_DIR, YEAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "comments = read_comm_csvs(comment_paths_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "comments_clean = clean_comments(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = comments_clean['body'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{OUT_DIR}docs_clean/diac_{YEAR}.pickle', 'wb') as fp:\n",
    "    pickle.dump(docs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{OUT_DIR}docs_clean/diac_{YEAR}.pickle', 'rb') as fp:\n",
    "    docs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-driving",
   "metadata": {},
   "source": [
    "#### Create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = train_model(corpus, EPOCHS=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-flash",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'{OUT_DIR}models/{YEAR}_ep-20.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-necessity",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2019 = Word2Vec.load(f'{OUT_DIR}models/2019_ep-20.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2020 = Word2Vec.load(f'{OUT_DIR}models/2020_ep-20.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-blank",
   "metadata": {},
   "source": [
    "### Align models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2019_vocab = len(model_2019.wv.key_to_index)\n",
    "model_2020_vocab = len(model_2020.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-awareness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190756 190756\n",
      "190756 190756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x110df3160>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smart_procrustes_align_gensim(model_2019, model_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model_2019.wv.key_to_index) == len(model_2020.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-cutting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>252564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>277707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intersection</td>\n",
       "      <td>190756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model   Words\n",
       "0          2019  252564\n",
       "1          2020  277707\n",
       "2  intersection  190756"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_vocab = pd.DataFrame(\n",
    "    columns=['Model', 'Words'],\n",
    "    data=[\n",
    "        ['2019', model_2019_vocab],\n",
    "        ['2020', model_2020_vocab],\n",
    "        ['intersection', len(model_2019.wv.key_to_index)]\n",
    "    ],\n",
    ")\n",
    "\n",
    "models_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_vocab.to_csv(f'{OUT_DIR}models_vocab.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-lighting",
   "metadata": {},
   "source": [
    "### Measure distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = measure_distances(model_2019, model_2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: filter by true type frequency; `Gensim`'s type frequency seems incorrect; it probably reflects frequency ranks instead of total counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist_lex = (pd.read_csv('../data/blacklist_lex.csv')\n",
    "    .query('Excl == True')\n",
    "    .loc[:, 'Lex']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-cannon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lex</th>\n",
       "      <th>dist_sem</th>\n",
       "      <th>freq_1</th>\n",
       "      <th>freq_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lockdowns</td>\n",
       "      <td>1.016951</td>\n",
       "      <td>940</td>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maskless</td>\n",
       "      <td>0.996101</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sunsetting</td>\n",
       "      <td>0.996084</td>\n",
       "      <td>111</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>childe</td>\n",
       "      <td>0.980564</td>\n",
       "      <td>209</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>megalodon</td>\n",
       "      <td>0.975273</td>\n",
       "      <td>752</td>\n",
       "      <td>792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>newf</td>\n",
       "      <td>0.962381</td>\n",
       "      <td>107</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>corona</td>\n",
       "      <td>0.926739</td>\n",
       "      <td>3553</td>\n",
       "      <td>3684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>filtrate</td>\n",
       "      <td>0.918609</td>\n",
       "      <td>102</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>chaz</td>\n",
       "      <td>0.899856</td>\n",
       "      <td>190</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>klee</td>\n",
       "      <td>0.888728</td>\n",
       "      <td>161</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rona</td>\n",
       "      <td>0.886363</td>\n",
       "      <td>409</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cerb</td>\n",
       "      <td>0.869179</td>\n",
       "      <td>315</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rittenhouse</td>\n",
       "      <td>0.866206</td>\n",
       "      <td>181</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>vacuo</td>\n",
       "      <td>0.862142</td>\n",
       "      <td>170</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>moderna</td>\n",
       "      <td>0.843465</td>\n",
       "      <td>199</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pandemic</td>\n",
       "      <td>0.837175</td>\n",
       "      <td>9504</td>\n",
       "      <td>9957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>spreader</td>\n",
       "      <td>0.835257</td>\n",
       "      <td>164</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>distancing</td>\n",
       "      <td>0.833737</td>\n",
       "      <td>2910</td>\n",
       "      <td>3038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sars</td>\n",
       "      <td>0.827039</td>\n",
       "      <td>880</td>\n",
       "      <td>924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>quarantines</td>\n",
       "      <td>0.820280</td>\n",
       "      <td>148</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            lex  dist_sem  freq_1  freq_2\n",
       "0     lockdowns  1.016951     940     991\n",
       "1      maskless  0.996101     118     127\n",
       "2    sunsetting  0.996084     111     120\n",
       "3        childe  0.980564     209     222\n",
       "4     megalodon  0.975273     752     792\n",
       "5          newf  0.962381     107     115\n",
       "6        corona  0.926739    3553    3684\n",
       "7      filtrate  0.918609     102     110\n",
       "8          chaz  0.899856     190     202\n",
       "9          klee  0.888728     161     173\n",
       "10         rona  0.886363     409     433\n",
       "11         cerb  0.869179     315     333\n",
       "12  rittenhouse  0.866206     181     194\n",
       "13        vacuo  0.862142     170     181\n",
       "14      moderna  0.843465     199     211\n",
       "15     pandemic  0.837175    9504    9957\n",
       "16     spreader  0.835257     164     175\n",
       "17   distancing  0.833737    2910    3038\n",
       "18         sars  0.827039     880     924\n",
       "19  quarantines  0.820280     148     159"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 20\n",
    "freq_min = 100\n",
    "\n",
    "sem_change_cands = (distances\\\n",
    "    .query('freq_1 > @freq_min and freq_2 > @freq_min')\n",
    "    .query('lex.str.isalpha() == True')\n",
    "    .query('lex.str.len() > 3')\n",
    "    .query('lex not in @blacklist_lex')\n",
    "    .nlargest(k, 'dist_sem')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "sem_change_cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_change_cands_out = (sem_change_cands\n",
    "    .nlargest(100, 'dist_sem')\n",
    "    .assign(index_1 = lambda df: df.index + 1)\n",
    "    .assign(dist_sem = lambda df: df['dist_sem'].round(2))\n",
    "    .assign(dist_sem = lambda df: df['dist_sem'].apply('{:.2f}'.format))\n",
    "    .rename({'index_1': '', 'lex': 'Lexeme', 'dist_sem': 'SemDist'}, axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_change_cands_out.to_csv(\n",
    "        f'{OUT_DIR}sem_change_cands.csv',\n",
    "        columns=['', 'Lexeme', 'SemDist'],\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-tomato",
   "metadata": {},
   "source": [
    "### Inspect nearest neighbours of lexemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEX_NBS = 'ahahahah'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-majority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>lex</th>\n",
       "      <th>similarity</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>hahahha</td>\n",
       "      <td>0.455687</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ahaha</td>\n",
       "      <td>0.455320</td>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>hahaha</td>\n",
       "      <td>0.441289</td>\n",
       "      <td>6690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ahha</td>\n",
       "      <td>0.436381</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>myyyy</td>\n",
       "      <td>0.434361</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>funni</td>\n",
       "      <td>0.433964</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>hahahah</td>\n",
       "      <td>0.432263</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>yeaaaa</td>\n",
       "      <td>0.429689</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>yess</td>\n",
       "      <td>0.429398</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>woooow</td>\n",
       "      <td>0.427130</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model      lex  similarity  freq\n",
       "0      1  hahahha    0.455687    76\n",
       "1      1    ahaha    0.455320   668\n",
       "2      1   hahaha    0.441289  6690\n",
       "3      1     ahha    0.436381    43\n",
       "4      1    myyyy    0.434361    14\n",
       "5      1    funni    0.433964    49\n",
       "6      1  hahahah    0.432263   496\n",
       "7      1   yeaaaa    0.429689    33\n",
       "8      1     yess    0.429398   320\n",
       "9      1   woooow    0.427130    46"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>lex</th>\n",
       "      <th>similarity</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>2</td>\n",
       "      <td>wiki_rule_2</td>\n",
       "      <td>0.389549</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>2</td>\n",
       "      <td>jk</td>\n",
       "      <td>0.381757</td>\n",
       "      <td>2248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>2</td>\n",
       "      <td>dqw4w9wgxcq</td>\n",
       "      <td>0.348253</td>\n",
       "      <td>763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>2</td>\n",
       "      <td>wiki_rule_b</td>\n",
       "      <td>0.346046</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>2</td>\n",
       "      <td>20enabled</td>\n",
       "      <td>0.326490</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>2</td>\n",
       "      <td>20questions</td>\n",
       "      <td>0.314265</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100006</th>\n",
       "      <td>2</td>\n",
       "      <td>flowerboy</td>\n",
       "      <td>0.309573</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100007</th>\n",
       "      <td>2</td>\n",
       "      <td>_love_</td>\n",
       "      <td>0.307687</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100008</th>\n",
       "      <td>2</td>\n",
       "      <td>subed</td>\n",
       "      <td>0.306114</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100009</th>\n",
       "      <td>2</td>\n",
       "      <td>420th</td>\n",
       "      <td>0.303107</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model          lex  similarity  freq\n",
       "100000      2  wiki_rule_2    0.389549   127\n",
       "100001      2           jk    0.381757  2248\n",
       "100002      2  dqw4w9wgxcq    0.348253   763\n",
       "100003      2  wiki_rule_b    0.346046    12\n",
       "100004      2    20enabled    0.326490   476\n",
       "100005      2  20questions    0.314265   513\n",
       "100006      2    flowerboy    0.309573    11\n",
       "100007      2       _love_    0.307687    21\n",
       "100008      2        subed    0.306114    21\n",
       "100009      2        420th    0.303107    22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbs_model_1, nbs_model_2 = get_nearest_neighbours_models(\n",
    "    lex=LEX_NBS, \n",
    "    freq_min=1,\n",
    "    model_1=model_2019, \n",
    "    model_2=model_2020,\n",
    "    k=10\n",
    ")\n",
    "\n",
    "display(\n",
    "    nbs_model_1,\n",
    "    nbs_model_2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not related to Covid:\n",
    "\n",
    "- sunsetting: > gaming-related meaning in 2020\n",
    "- childe: > gaming-related proper name in 2020\n",
    "- megalodon: > gaming-related proper name in 2020\n",
    "- newf: (derogatory) slang term for people from Newfoundland (Canada)\n",
    "- chaz: > Capitol Hill Autonomous Zone (CHAZ)\n",
    "- klee: > computer game character, proper name\n",
    "- rittenhouse: whiskey brand > proper name, involved in shooting related to BLM protests\n",
    "\n",
    "Related to Covid:\n",
    "\n",
    "- cerb: > Canada Emergency Response Benefit for Covid\n",
    "- vacuo: > medical term, 'vacuum'\n",
    "- moderna: > vaccine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social semantic variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-chorus",
   "metadata": {},
   "source": [
    "### Inspect subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-coordinator",
   "metadata": {},
   "source": [
    "#### read comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_dir_path = Path('../data/comments/lexeme/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_paths = list(comments_dir_path.glob(f'Covid*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "comments = read_comm_csvs(comments_paths)\n",
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: filter comments\n",
    "\n",
    "- [ ] remove duplicates\n",
    "- [ ] remove bots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-rocket",
   "metadata": {},
   "source": [
    "#### get subreddit counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "subr_counts = get_subr_counts(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "subr_counts_plt = plot_subr_counts(subr_counts, k=20)\n",
    "subr_counts_plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "subr_counts_plt.save(f'{OUT_DIR}subr_counts.png', scale_factor=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-kentucky",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENTS_DIR_SUBR = '../data/comments/subr/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBR = 'conspiracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths = get_comments_paths_subr(COMMENTS_DIR_SUBR, SUBR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "comments = read_comm_csvs(fpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "comments_clean = clean_comments(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = comments_clean['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docs.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{OUT_DIR}docs_clean/subr_{SUBR}.pickle', 'wb') as fp:\n",
    "    pickle.dump(docs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{OUT_DIR}docs_clean/subr_{SUBR}.pickle', 'rb') as fp:\n",
    "    docs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus information\n",
    "\n",
    "| Subreddit          | Comments  | DateFirst  | DateLast   |\n",
    "|:-------------------|---------: |:-----------|:-----------|\n",
    "| LockdownSkepticism |   520,392 | 2020-03-26 | 2020-12-27 |  \n",
    "| Coronavirus        | 4,121,144 | 2020-01-21 | 2020-12-27 |\n",
    "| conspiracy         | 3,973,514 | 2020-01-01 | 2020-12-27 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = train_model(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'{OUT_DIR}models/{SUBR}.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-necessity",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-artist",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBRS = ['Coronavirus', 'conspiracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Word2Vec.load(f'{OUT_DIR}models/{SUBRS[0]}.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Word2Vec.load(f'{OUT_DIR}models/{SUBRS[1]}.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-blank",
   "metadata": {},
   "source": [
    "### Align models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_vocab = len(model_1.wv.key_to_index)\n",
    "model_2_vocab = len(model_2.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_procrustes_align_gensim(model_1, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model_1.wv.key_to_index) == len(model_2.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_vocab = pd.DataFrame(\n",
    "    columns=['Model', 'Words'],\n",
    "    data=[\n",
    "        [SUBRS[0], model_1_vocab],\n",
    "        [SUBRS[1], model_2_vocab],\n",
    "        ['intersection', len(model_1.wv.key_to_index)]\n",
    "    ],\n",
    ")\n",
    "\n",
    "models_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_vocab.to_csv(f'{OUT_DIR}models_subrs_vocab.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-lighting",
   "metadata": {},
   "source": [
    "### Measure distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = measure_distances(model_1, model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### words that differ the most between both communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_min = 100\n",
    "\n",
    "distances\\\n",
    "    .query('freq_1 > @freq_min and freq_2 > @freq_min')\\\n",
    "    .sort_values('dist_sem', ascending=False)\\\n",
    "    .head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nearest neighbours for target lexemes in both communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEX = 'vaccine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbs_model_1, nbs_model_2 = get_nearest_neighbours_models(\n",
    "    lex=LEX, \n",
    "    freq_min=100,\n",
    "    model_1=model_1, \n",
    "    model_2=model_2,\n",
    "    k=10\n",
    ")\n",
    "\n",
    "display(nbs_model_1, nbs_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### biggest discrepancies in nearest neighbours for target lexemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbs_model_1, nbs_model_2 = get_nearest_neighbours_models(\n",
    "    lex=LEX, \n",
    "    freq_min=150,\n",
    "    model_1=model_1, \n",
    "    model_2=model_2,\n",
    "    k=100_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbs_diffs = pd.merge(\n",
    "    nbs_model_1, nbs_model_2, \n",
    "    on='lex',\n",
    "    suffixes = ('_1', '_2')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbs_diffs = nbs_diffs\\\n",
    "    .assign(sim_diff = abs(nbs_diffs['similarity_1'] - nbs_diffs['similarity_2']))\\\n",
    "    .sort_values('sim_diff', ascending=False)\\\n",
    "    .reset_index(drop=True)\\\n",
    "    .query('lex.str.len() >= 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 10\n",
    "\n",
    "subr_1_nbs = nbs_diffs\\\n",
    "    .query('similarity_1 > similarity_2')\\\n",
    "    .nlargest(topn, 'sim_diff')\n",
    "\n",
    "subr_2_nbs = nbs_diffs\\\n",
    "    .query('similarity_2 > similarity_1')\\\n",
    "    .nlargest(topn, 'sim_diff')\n",
    "\n",
    "display(subr_1_nbs, subr_2_nbs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
