{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp type_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    ['A', 'test', 'sentence'],\n",
    "    ['Another', 'test', 'sentence']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Corpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self, docs):\n",
    "        self.docs_clean = docs\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc in self.docs_clean:\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def train_model(corpus,\n",
    "              MIN_COUNT=5,\n",
    "              SIZE=300,\n",
    "              WORKERS=8,\n",
    "              WINDOW=5):\n",
    "    model = Word2Vec(\n",
    "        corpus,\n",
    "        min_count=MIN_COUNT,\n",
    "        vector_size=SIZE,\n",
    "        workers=WORKERS,\n",
    "        window=WINDOW\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(corpus, MIN_COUNT=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## align models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = train_model(corpus=[['The', 'bank', 'of', 'the', 'river']], MIN_COUNT=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = train_model(corpus=[['The', 'bank', 'of', 'England']], MIN_COUNT=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model_1.wv.key_to_index) != len(model_2.wv.vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def intersection_align_gensim(m1, m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.index_to_key)\n",
    "    vocab_m2 = set(m2.wv.index_to_key)\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1 & vocab_m2\n",
    "    if words: common_vocab &= set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n",
    "    # print(len(common_vocab))\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1, m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.key_to_index[w] for w in common_vocab]\n",
    "        old_arr = m.wv.vectors\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.vectors = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        new_key_to_index = {}\n",
    "        new_index_to_key = []\n",
    "        for new_index, key in enumerate(common_vocab):\n",
    "            new_key_to_index[key] = new_index\n",
    "            new_index_to_key.append(key)\n",
    "        m.wv.key_to_index = new_key_to_index\n",
    "        m.wv.index_to_key = new_index_to_key\n",
    "        \n",
    "        print(len(m.wv.key_to_index), len(m.wv.vectors))\n",
    "        \n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"\n",
    "    Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf\n",
    "    Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "        \n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the (normalized) embedding matrices\n",
    "    base_vecs = in_base_embed.wv.get_normed_vectors()\n",
    "    other_vecs = in_other_embed.wv.get_normed_vectors()\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one, i.e. multiplying the embedding matrix by \"ortho\"\n",
    "    other_embed.wv.vectors = (other_embed.wv.vectors).dot(ortho)    \n",
    "    \n",
    "    return other_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-awareness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n",
      "3 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x14e992bc0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smart_procrustes_align_gensim(model_1, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model_1.wv.key_to_index) == len(model_2.wv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## measure distances between types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def measure_distances(model_1, model_2):\n",
    "    distances = pd.DataFrame(\n",
    "        columns=('lex', 'dist_sem', \"freq_1\", \"freq_2\"),\n",
    "        data=(\n",
    "            #[w, spatial.distance.euclidean(model_1.wv[w], model_2.wv[w]),\n",
    "            #[w, np.sum(model_1.wv[w] * model_2.wv[w]) / (np.linalg.norm(model_1.wv[w]) * np.linalg.norm(model_2.wv[w])),\n",
    "            [w, spatial.distance.cosine(model_1.wv[w], model_2.wv[w]),\n",
    "             model_1.wv.get_vecattr(w, \"count\"),\n",
    "             model_2.wv.get_vecattr(w, \"count\")\n",
    "             ] for w in model_1.wv.index_to_key\n",
    "        )\n",
    "    )\n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = measure_distances(model_1, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lex</th>\n",
       "      <th>dist_sem</th>\n",
       "      <th>freq_1</th>\n",
       "      <th>freq_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bank</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lex  dist_sem  freq_1  freq_2\n",
       "0   The  0.001182       1       1\n",
       "1  bank  0.000995       1       1\n",
       "2    of  0.000375       1       1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances\\\n",
    "    .sort_values('dist_sem', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get nearest neighbours of lexemes for 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_nearest_neighbours_models(lex, freq_min, model_1, model_2, topn=100_000, k=10):\n",
    "    nbs = []\n",
    "    for count, model in enumerate([model_1, model_2]):\n",
    "        for nb, dist in model.wv.most_similar(lex, topn=topn):\n",
    "            if model.wv.get_vecattr(nb, 'count') > freq_min:\n",
    "                d = {}\n",
    "                d['model'] = count + 1\n",
    "                d['lex'] = nb\n",
    "                d['similarity'] = dist\n",
    "                d['freq'] = model.wv.get_vecattr(nb, \"count\")\n",
    "                nbs.append(d)\n",
    "    nbs_df = pd.DataFrame(nbs)\n",
    "    nbs_df = nbs_df\\\n",
    "        .query('freq > @freq_min')\\\n",
    "        .groupby('model', group_keys=False)\\\n",
    "        .apply(lambda group: group.nlargest(k, 'similarity'))\n",
    "    nbs_model_1 = nbs_df.query('model == 1')\n",
    "    nbs_model_2 = nbs_df.query('model == 2')\n",
    "    return nbs_model_1, nbs_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit ('neocov': conda)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
