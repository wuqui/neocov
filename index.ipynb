{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neocov.read_data import *\n",
    "from neocov.preproc import *\n",
    "from neocov.type_emb import *\n",
    "from neocov.communities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/'\n",
    "COMMENTS_DIAC_DIR = f'{DATA_DIR}comments/by_date/'\n",
    "OUT_DIR = '../out/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeoCov\n",
    "\n",
    "> Semantic change and social semantic variation of Covid-related English neologisms on Reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = '2020'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_paths_year = get_comments_paths_year(COMMENTS_DIAC_DIR, YEAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "comments = read_comm_csvs(comment_paths_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "comments_clean = clean_comments(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = comments_clean['body'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{OUT_DIR}docs_clean/diac_{YEAR}.pickle', 'wb') as fp:\n",
    "    pickle.dump(docs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{OUT_DIR}docs_clean/diac_{YEAR}.pickle', 'rb') as fp:\n",
    "    docs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-driving",
   "metadata": {},
   "source": [
    "#### Create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = train_model(corpus, EPOCHS=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-flash",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'{OUT_DIR}models/{YEAR}_ep-20.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-necessity",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2019 = Word2Vec.load(f'{OUT_DIR}models/2019_ep-20.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2020 = Word2Vec.load(f'{OUT_DIR}models/2020_ep-20.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-blank",
   "metadata": {},
   "source": [
    "### Align models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2019_vocab = len(model_2019.wv.key_to_index)\n",
    "model_2020_vocab = len(model_2020.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_procrustes_align_gensim(model_2019, model_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model_2019.wv.key_to_index) == len(model_2020.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_vocab = pd.DataFrame(\n",
    "    columns=['Model', 'Words'],\n",
    "    data=[\n",
    "        ['2019', model_2019_vocab],\n",
    "        ['2020', model_2020_vocab],\n",
    "        ['intersection', len(model_2019.wv.key_to_index)]\n",
    "    ],\n",
    ")\n",
    "\n",
    "models_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_vocab.to_csv(f'{OUT_DIR}models_vocab.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-lighting",
   "metadata": {},
   "source": [
    "### Measure distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = measure_distances(model_2019, model_2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: filter by true type frequency; `Gensim`'s type frequency seems incorrect; it probably reflects frequency ranks instead of total counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "freq_min = 100\n",
    "\n",
    "sem_change_cands = (distances\\\n",
    "    .query('freq_1 > @freq_min and freq_2 > @freq_min')\n",
    "    .query('lex.str.isalpha() == True')\\\n",
    "    .query('lex.str.len() > 3')\\\n",
    "    .nlargest(k, 'dist_sem')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "sem_change_cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_change_cands_out = sem_change_cands\\\n",
    "    .nlargest(100, 'dist_sem')\\\n",
    "    .assign(index_1 = lambda df: df.index + 1)\\\n",
    "    .assign(dist_sem = lambda df: df['dist_sem'].round(2))\\\n",
    "    .assign(dist_sem = lambda df: df['dist_sem'].apply('{:.2f}'.format))\\\n",
    "    .rename({'index_1': '', 'lex': 'Lexeme', 'dist_sem': 'SemDist'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_change_cands_out.to_csv(\n",
    "        f'{OUT_DIR}sem_change_cands.csv',\n",
    "        columns=['', 'Lexeme', 'SemDist'],\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-tomato",
   "metadata": {},
   "source": [
    "### Inspect nearest neighbours of lexemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEX_NBS = 'chaz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbs_model_1, nbs_model_2 = get_nearest_neighbours_models(\n",
    "    lex=LEX_NBS, \n",
    "    freq_min=1,\n",
    "    model_1=model_2019, \n",
    "    model_2=model_2020,\n",
    "    k=10\n",
    ")\n",
    "\n",
    "display(\n",
    "    nbs_model_1,\n",
    "    nbs_model_2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social semantic variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-chorus",
   "metadata": {},
   "source": [
    "### Inspect subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-coordinator",
   "metadata": {},
   "source": [
    "#### read comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_dir_path = Path('../data/comments/lexeme/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_paths = list(comments_dir_path.glob(f'Covid*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "comments = read_comm_csvs(comments_paths)\n",
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: filter comments\n",
    "\n",
    "- [ ] remove duplicates\n",
    "- [ ] remove bots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-rocket",
   "metadata": {},
   "source": [
    "#### get subreddit counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "subr_counts = get_subr_counts(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "subr_counts_plt = plot_subr_counts(subr_counts, k=20)\n",
    "subr_counts_plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "subr_counts_plt.save(f'{OUT_DIR}subr_counts.png', scale_factor=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-kentucky",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENTS_DIR_SUBR = '../data/comments/subr/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBR = 'conspiracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths = get_comments_paths_subr(COMMENTS_DIR_SUBR, SUBR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "comments = read_comm_csvs(fpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "comments_clean = clean_comments(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = comments_clean['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docs.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{OUT_DIR}docs_clean/subr_{SUBR}.pickle', 'wb') as fp:\n",
    "    pickle.dump(docs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{OUT_DIR}docs_clean/subr_{SUBR}.pickle', 'rb') as fp:\n",
    "    docs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus information\n",
    "\n",
    "| Subreddit          | Comments  | DateFirst  | DateLast   |\n",
    "|:-------------------|---------: |:-----------|:-----------|\n",
    "| LockdownSkepticism |   520,392 | 2020-03-26 | 2020-12-27 |  \n",
    "| Coronavirus        | 4,121,144 | 2020-01-21 | 2020-12-27 |\n",
    "| conspiracy         | 3,973,514 | 2020-01-01 | 2020-12-27 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = train_model(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'{OUT_DIR}models/{SUBR}.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-necessity",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-artist",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBRS = ['Coronavirus', 'conspiracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Word2Vec.load(f'{OUT_DIR}models/{SUBRS[0]}.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Word2Vec.load(f'{OUT_DIR}models/{SUBRS[1]}.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-blank",
   "metadata": {},
   "source": [
    "### Align models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_vocab = len(model_1.wv.key_to_index)\n",
    "model_2_vocab = len(model_2.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_procrustes_align_gensim(model_1, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model_1.wv.key_to_index) == len(model_2.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_vocab = pd.DataFrame(\n",
    "    columns=['Model', 'Words'],\n",
    "    data=[\n",
    "        [SUBRS[0], model_1_vocab],\n",
    "        [SUBRS[1], model_2_vocab],\n",
    "        ['intersection', len(model_1.wv.key_to_index)]\n",
    "    ],\n",
    ")\n",
    "\n",
    "models_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_vocab.to_csv(f'{OUT_DIR}models_subrs_vocab.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-lighting",
   "metadata": {},
   "source": [
    "### Measure distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = measure_distances(model_1, model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### words that differ the most between both communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_min = 100\n",
    "\n",
    "distances\\\n",
    "    .query('freq_1 > @freq_min and freq_2 > @freq_min')\\\n",
    "    .sort_values('dist_sem', ascending=False)\\\n",
    "    .head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nearest neighbours for target lexemes in both communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEX = 'vaccine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbs_model_1, nbs_model_2 = get_nearest_neighbours_models(\n",
    "    lex=LEX, \n",
    "    freq_min=100,\n",
    "    model_1=model_1, \n",
    "    model_2=model_2,\n",
    "    k=10\n",
    ")\n",
    "\n",
    "display(nbs_model_1, nbs_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### biggest discrepancies in nearest neighbours for target lexemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbs_model_1, nbs_model_2 = get_nearest_neighbours_models(\n",
    "    lex=LEX, \n",
    "    freq_min=150,\n",
    "    model_1=model_1, \n",
    "    model_2=model_2,\n",
    "    k=100_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbs_diffs = pd.merge(\n",
    "    nbs_model_1, nbs_model_2, \n",
    "    on='lex',\n",
    "    suffixes = ('_1', '_2')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbs_diffs = nbs_diffs\\\n",
    "    .assign(sim_diff = abs(nbs_diffs['similarity_1'] - nbs_diffs['similarity_2']))\\\n",
    "    .sort_values('sim_diff', ascending=False)\\\n",
    "    .reset_index(drop=True)\\\n",
    "    .query('lex.str.len() >= 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 10\n",
    "\n",
    "subr_1_nbs = nbs_diffs\\\n",
    "    .query('similarity_1 > similarity_2')\\\n",
    "    .nlargest(topn, 'sim_diff')\n",
    "\n",
    "subr_2_nbs = nbs_diffs\\\n",
    "    .query('similarity_2 > similarity_1')\\\n",
    "    .nlargest(topn, 'sim_diff')\n",
    "\n",
    "display(subr_1_nbs, subr_2_nbs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
